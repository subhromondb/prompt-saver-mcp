"""OpenAI client for prompt analysis and generation."""

import json
import logging
from typing import Dict, List, Optional

from openai import OpenAI

from prompt_saver_mcp.config import config

logger = logging.getLogger(__name__)

USE_CASES = ["code-gen", "text-gen", "data-analysis", "creative", "general"]


class OpenAIClient:
    """OpenAI client for analyzing conversations and generating prompts."""

    def __init__(self):
        """Initialize OpenAI client."""
        if not config.OPENAI_API_KEY:
            raise ValueError("OPENAI_API_KEY is required")
        self.client = OpenAI(api_key=config.OPENAI_API_KEY)
        self.model = config.OPENAI_MODEL

    def analyze_conversation(
        self, conversation_messages: List[Dict], task_description: Optional[str] = None
    ) -> Dict[str, str]:
        """
        Analyze a conversation thread and extract key information.

        Args:
            conversation_messages: List of conversation messages
            task_description: Optional description of the task

        Returns:
            Dictionary with use_case, summary, and prompt_template
        """
        try:
            # Format conversation for analysis
            conversation_text = self._format_conversation(conversation_messages)

            system_prompt = """You are an expert at analyzing conversation threads and extracting comprehensive, reusable prompt patterns.

Your task is to:
1. Categorize the conversation into one of these use cases: code-gen, text-gen, data-analysis, creative, general
2. Create a detailed summary of what the conversation accomplished
3. Generate a comprehensive, reusable prompt template in markdown format that captures ALL the key patterns, steps, examples, and details

The prompt template should be:
- COMPREHENSIVE and DETAILED - include specific examples, code snippets, file paths, commands, and concrete patterns from the conversation
- Well-structured with clear sections and subsections
- Include specific examples alongside placeholders for variable inputs
- Preserve important details like exact file paths, SQL queries, specific commands, error messages, and solutions
- Capture the complete problem-solving approach with all nuances
- Include a detailed "Overview" section at the beginning explaining the context and what this prompt helps accomplish
- Be VERBOSE - include all relevant details that would help someone replicate the success

IMPORTANT: Do NOT oversimplify or make it too generic. Include:
- Specific code examples from the conversation
- Exact file paths mentioned
- Complete SQL queries or commands
- Detailed step-by-step procedures
- Common pitfalls and solutions
- Validation checklists
- Any specific technical details discussed

Return your response as a JSON object with these keys:
- use_case: one of the use case categories
- summary: a detailed summary (can be multiple sentences, include key details)
- prompt_template: the comprehensive markdown-formatted prompt template with overview, examples, and details
- history: a detailed summary of the steps taken and end result (include specific details)
"""

            user_prompt = f"""Analyze this conversation thread:

{conversation_text}

{f'Task description: {task_description}' if task_description else ''}

Extract the reusable prompt pattern and return as JSON."""

            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt},
                ],
                response_format={"type": "json_object"},
                temperature=0.7,
            )

            result_text = response.choices[0].message.content
            if not result_text:
                raise ValueError("Empty response from OpenAI")

            result = json.loads(result_text)

            # Validate use_case
            if result.get("use_case") not in USE_CASES:
                logger.warning(f"Invalid use_case {result.get('use_case')}, defaulting to 'general'")
                result["use_case"] = "general"

            return {
                "use_case": result.get("use_case", "general"),
                "summary": result.get("summary", ""),
                "prompt_template": result.get("prompt_template", ""),
                "history": result.get("history", ""),
            }
        except Exception as e:
            logger.error(f"Failed to analyze conversation: {e}")
            raise

    def improve_prompt_from_feedback(
        self, current_prompt: str, feedback: str, conversation_context: Optional[str] = None
    ) -> str:
        """
        Improve a prompt based on user feedback.

        Args:
            current_prompt: The current prompt template
            feedback: User feedback on the prompt
            conversation_context: Optional context about how the prompt was used

        Returns:
            Improved prompt template
        """
        try:
            system_prompt = """You are an expert at improving prompts based on feedback.

Your task is to take the current prompt and user feedback, then generate an improved version that:
- Addresses the feedback points comprehensively
- Maintains or enhances the detailed structure and approach
- Enhances clarity and effectiveness with MORE detail, not less
- Preserves or adds specific examples, code snippets, file paths, and concrete patterns
- Remains reusable but highly detailed and informative

IMPORTANT: When user asks for more details, examples, or verbosity - ADD MORE, don't simplify.
Include specific examples, code snippets, exact commands, and detailed explanations.

Return only the improved prompt template in markdown format, without any additional commentary."""

            user_prompt = f"""Current prompt template:

{current_prompt}

User feedback:
{feedback}

{f'Context: {conversation_context}' if conversation_context else ''}

Generate an improved version of this prompt."""

            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt},
                ],
                temperature=0.7,
            )

            improved_prompt = response.choices[0].message.content
            if not improved_prompt:
                raise ValueError("Empty response from OpenAI")

            return improved_prompt.strip()
        except Exception as e:
            logger.error(f"Failed to improve prompt: {e}")
            raise

    def _format_conversation(self, messages: List[Dict]) -> str:
        """
        Format conversation messages into a readable string.

        Args:
            messages: List of message dictionaries

        Returns:
            Formatted conversation string
        """
        formatted = []
        for msg in messages:
            role = msg.get("role", "unknown")
            content = msg.get("content", "")
            formatted.append(f"{role.upper()}: {content}")
        return "\n\n".join(formatted)


# Global OpenAI client instance (lazy initialization)
_openai_client: Optional[OpenAIClient] = None


def get_openai_client() -> OpenAIClient:
    """Get or create the global OpenAI client instance."""
    global _openai_client
    if _openai_client is None:
        _openai_client = OpenAIClient()
    return _openai_client


# Create a simple object that delegates to get_openai_client()
class OpenAIClientProxy:
    """Proxy for lazy-loaded OpenAI client."""

    def __getattr__(self, name):
        return getattr(get_openai_client(), name)


openai_client = OpenAIClientProxy()

